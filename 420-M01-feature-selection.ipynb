{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capstone - Milestone 1\n",
    "\n",
    "\n",
    "\n",
    "In our first step, we scoped out our client's diaper manufacturing process. We identified potential defects in the manufacturing process and what sensors/metrics to monitor. We now have a data set with nearly 591 unlabeled features we will be using to predict defects in the process.\n",
    "\n",
    "The goals of phase 1 of our project are listed below:\n",
    "- Join label and feature data\n",
    "- Impute missing values\n",
    "- Remove unnecessary columns\n",
    "- Perform feature engineering\n",
    "- Check and solve for class imbalance\n",
    "- Apply feature selection methods to reduce data dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1567, 590)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "data = np.loadtxt('data/secom.data')\n",
    "\n",
    "# Data here is a numpy array (2 dimensional)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>580</th>\n",
       "      <th>581</th>\n",
       "      <th>582</th>\n",
       "      <th>583</th>\n",
       "      <th>584</th>\n",
       "      <th>585</th>\n",
       "      <th>586</th>\n",
       "      <th>587</th>\n",
       "      <th>588</th>\n",
       "      <th>589</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3030.93</td>\n",
       "      <td>2564.00</td>\n",
       "      <td>2187.7333</td>\n",
       "      <td>1411.1265</td>\n",
       "      <td>1.3602</td>\n",
       "      <td>100.0</td>\n",
       "      <td>97.6133</td>\n",
       "      <td>0.1242</td>\n",
       "      <td>1.5005</td>\n",
       "      <td>0.0162</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5005</td>\n",
       "      <td>0.0118</td>\n",
       "      <td>0.0035</td>\n",
       "      <td>2.3630</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3095.78</td>\n",
       "      <td>2465.14</td>\n",
       "      <td>2230.4222</td>\n",
       "      <td>1463.6606</td>\n",
       "      <td>0.8294</td>\n",
       "      <td>100.0</td>\n",
       "      <td>102.3433</td>\n",
       "      <td>0.1247</td>\n",
       "      <td>1.4966</td>\n",
       "      <td>-0.0005</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>208.2045</td>\n",
       "      <td>0.5019</td>\n",
       "      <td>0.0223</td>\n",
       "      <td>0.0055</td>\n",
       "      <td>4.4447</td>\n",
       "      <td>0.0096</td>\n",
       "      <td>0.0201</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>208.2045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2932.61</td>\n",
       "      <td>2559.94</td>\n",
       "      <td>2186.4111</td>\n",
       "      <td>1698.0172</td>\n",
       "      <td>1.5102</td>\n",
       "      <td>100.0</td>\n",
       "      <td>95.4878</td>\n",
       "      <td>0.1241</td>\n",
       "      <td>1.4436</td>\n",
       "      <td>0.0041</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0148</td>\n",
       "      <td>82.8602</td>\n",
       "      <td>0.4958</td>\n",
       "      <td>0.0157</td>\n",
       "      <td>0.0039</td>\n",
       "      <td>3.1745</td>\n",
       "      <td>0.0584</td>\n",
       "      <td>0.0484</td>\n",
       "      <td>0.0148</td>\n",
       "      <td>82.8602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2988.72</td>\n",
       "      <td>2479.90</td>\n",
       "      <td>2199.0333</td>\n",
       "      <td>909.7926</td>\n",
       "      <td>1.3204</td>\n",
       "      <td>100.0</td>\n",
       "      <td>104.2367</td>\n",
       "      <td>0.1217</td>\n",
       "      <td>1.4882</td>\n",
       "      <td>-0.0124</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>73.8432</td>\n",
       "      <td>0.4990</td>\n",
       "      <td>0.0103</td>\n",
       "      <td>0.0025</td>\n",
       "      <td>2.0544</td>\n",
       "      <td>0.0202</td>\n",
       "      <td>0.0149</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>73.8432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3032.24</td>\n",
       "      <td>2502.87</td>\n",
       "      <td>2233.3667</td>\n",
       "      <td>1326.5200</td>\n",
       "      <td>1.5334</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.3967</td>\n",
       "      <td>0.1235</td>\n",
       "      <td>1.5031</td>\n",
       "      <td>-0.0031</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.4800</td>\n",
       "      <td>0.4766</td>\n",
       "      <td>0.1045</td>\n",
       "      <td>99.3032</td>\n",
       "      <td>0.0202</td>\n",
       "      <td>0.0149</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>73.8432</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 590 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0        1          2          3       4      5         6       7    \\\n",
       "0  3030.93  2564.00  2187.7333  1411.1265  1.3602  100.0   97.6133  0.1242   \n",
       "1  3095.78  2465.14  2230.4222  1463.6606  0.8294  100.0  102.3433  0.1247   \n",
       "2  2932.61  2559.94  2186.4111  1698.0172  1.5102  100.0   95.4878  0.1241   \n",
       "3  2988.72  2479.90  2199.0333   909.7926  1.3204  100.0  104.2367  0.1217   \n",
       "4  3032.24  2502.87  2233.3667  1326.5200  1.5334  100.0  100.3967  0.1235   \n",
       "\n",
       "      8       9    ...     580       581     582     583     584      585  \\\n",
       "0  1.5005  0.0162  ...     NaN       NaN  0.5005  0.0118  0.0035   2.3630   \n",
       "1  1.4966 -0.0005  ...  0.0060  208.2045  0.5019  0.0223  0.0055   4.4447   \n",
       "2  1.4436  0.0041  ...  0.0148   82.8602  0.4958  0.0157  0.0039   3.1745   \n",
       "3  1.4882 -0.0124  ...  0.0044   73.8432  0.4990  0.0103  0.0025   2.0544   \n",
       "4  1.5031 -0.0031  ...     NaN       NaN  0.4800  0.4766  0.1045  99.3032   \n",
       "\n",
       "      586     587     588       589  \n",
       "0     NaN     NaN     NaN       NaN  \n",
       "1  0.0096  0.0201  0.0060  208.2045  \n",
       "2  0.0584  0.0484  0.0148   82.8602  \n",
       "3  0.0202  0.0149  0.0044   73.8432  \n",
       "4  0.0202  0.0149  0.0044   73.8432  \n",
       "\n",
       "[5 rows x 590 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "secom = pd.DataFrame(data)\n",
    "secom.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-1 \"19/07/2008 11:55:00\"\\n', '-1 \"19/07/2008 12:32:00\"\\n', '1 \"19/07/2008 13:17:00\"\\n']\n"
     ]
    }
   ],
   "source": [
    "#read in labels data\n",
    "\n",
    "with open('data/secom_labels.data', 'r') as f:\n",
    "    labels = f.readlines()\n",
    "    \n",
    "print(labels[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data set needs to be cleaned. We'll start by splitting each value in the list using the spaces, then replacing the double quotes with a single quote and stripping any other unnecessary spaces. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split by spaces\n",
    "\n",
    "labels = [label.split(' ') for label in labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace double quotes and strip extra spaces\n",
    "\n",
    "labels = [[entry.strip().replace('\"', '') for entry in label] for label in labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>19/07/2008</td>\n",
       "      <td>11:55:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1</td>\n",
       "      <td>19/07/2008</td>\n",
       "      <td>12:32:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>19/07/2008</td>\n",
       "      <td>13:17:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1</td>\n",
       "      <td>19/07/2008</td>\n",
       "      <td>14:43:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1</td>\n",
       "      <td>19/07/2008</td>\n",
       "      <td>15:22:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  target        date      time\n",
       "0     -1  19/07/2008  11:55:00\n",
       "1     -1  19/07/2008  12:32:00\n",
       "2      1  19/07/2008  13:17:00\n",
       "3     -1  19/07/2008  14:43:00\n",
       "4     -1  19/07/2008  15:22:00"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convert to dataframe\n",
    "\n",
    "labels = pd.DataFrame(labels, columns = ['target', 'date', 'time'])\n",
    "labels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a clean data frame of the three columns from the labels data set. Let's join this to the feature set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>583</th>\n",
       "      <th>584</th>\n",
       "      <th>585</th>\n",
       "      <th>586</th>\n",
       "      <th>587</th>\n",
       "      <th>588</th>\n",
       "      <th>589</th>\n",
       "      <th>target</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3030.93</td>\n",
       "      <td>2564.00</td>\n",
       "      <td>2187.7333</td>\n",
       "      <td>1411.1265</td>\n",
       "      <td>1.3602</td>\n",
       "      <td>100.0</td>\n",
       "      <td>97.6133</td>\n",
       "      <td>0.1242</td>\n",
       "      <td>1.5005</td>\n",
       "      <td>0.0162</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0118</td>\n",
       "      <td>0.0035</td>\n",
       "      <td>2.3630</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "      <td>19/07/2008</td>\n",
       "      <td>11:55:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3095.78</td>\n",
       "      <td>2465.14</td>\n",
       "      <td>2230.4222</td>\n",
       "      <td>1463.6606</td>\n",
       "      <td>0.8294</td>\n",
       "      <td>100.0</td>\n",
       "      <td>102.3433</td>\n",
       "      <td>0.1247</td>\n",
       "      <td>1.4966</td>\n",
       "      <td>-0.0005</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0223</td>\n",
       "      <td>0.0055</td>\n",
       "      <td>4.4447</td>\n",
       "      <td>0.0096</td>\n",
       "      <td>0.0201</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>208.2045</td>\n",
       "      <td>-1</td>\n",
       "      <td>19/07/2008</td>\n",
       "      <td>12:32:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2932.61</td>\n",
       "      <td>2559.94</td>\n",
       "      <td>2186.4111</td>\n",
       "      <td>1698.0172</td>\n",
       "      <td>1.5102</td>\n",
       "      <td>100.0</td>\n",
       "      <td>95.4878</td>\n",
       "      <td>0.1241</td>\n",
       "      <td>1.4436</td>\n",
       "      <td>0.0041</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0157</td>\n",
       "      <td>0.0039</td>\n",
       "      <td>3.1745</td>\n",
       "      <td>0.0584</td>\n",
       "      <td>0.0484</td>\n",
       "      <td>0.0148</td>\n",
       "      <td>82.8602</td>\n",
       "      <td>1</td>\n",
       "      <td>19/07/2008</td>\n",
       "      <td>13:17:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2988.72</td>\n",
       "      <td>2479.90</td>\n",
       "      <td>2199.0333</td>\n",
       "      <td>909.7926</td>\n",
       "      <td>1.3204</td>\n",
       "      <td>100.0</td>\n",
       "      <td>104.2367</td>\n",
       "      <td>0.1217</td>\n",
       "      <td>1.4882</td>\n",
       "      <td>-0.0124</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0103</td>\n",
       "      <td>0.0025</td>\n",
       "      <td>2.0544</td>\n",
       "      <td>0.0202</td>\n",
       "      <td>0.0149</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>73.8432</td>\n",
       "      <td>-1</td>\n",
       "      <td>19/07/2008</td>\n",
       "      <td>14:43:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3032.24</td>\n",
       "      <td>2502.87</td>\n",
       "      <td>2233.3667</td>\n",
       "      <td>1326.5200</td>\n",
       "      <td>1.5334</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.3967</td>\n",
       "      <td>0.1235</td>\n",
       "      <td>1.5031</td>\n",
       "      <td>-0.0031</td>\n",
       "      <td>...</td>\n",
       "      <td>0.4766</td>\n",
       "      <td>0.1045</td>\n",
       "      <td>99.3032</td>\n",
       "      <td>0.0202</td>\n",
       "      <td>0.0149</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>73.8432</td>\n",
       "      <td>-1</td>\n",
       "      <td>19/07/2008</td>\n",
       "      <td>15:22:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 593 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0        1          2          3       4      5         6       7  \\\n",
       "0  3030.93  2564.00  2187.7333  1411.1265  1.3602  100.0   97.6133  0.1242   \n",
       "1  3095.78  2465.14  2230.4222  1463.6606  0.8294  100.0  102.3433  0.1247   \n",
       "2  2932.61  2559.94  2186.4111  1698.0172  1.5102  100.0   95.4878  0.1241   \n",
       "3  2988.72  2479.90  2199.0333   909.7926  1.3204  100.0  104.2367  0.1217   \n",
       "4  3032.24  2502.87  2233.3667  1326.5200  1.5334  100.0  100.3967  0.1235   \n",
       "\n",
       "        8       9  ...     583     584      585     586     587     588  \\\n",
       "0  1.5005  0.0162  ...  0.0118  0.0035   2.3630     NaN     NaN     NaN   \n",
       "1  1.4966 -0.0005  ...  0.0223  0.0055   4.4447  0.0096  0.0201  0.0060   \n",
       "2  1.4436  0.0041  ...  0.0157  0.0039   3.1745  0.0584  0.0484  0.0148   \n",
       "3  1.4882 -0.0124  ...  0.0103  0.0025   2.0544  0.0202  0.0149  0.0044   \n",
       "4  1.5031 -0.0031  ...  0.4766  0.1045  99.3032  0.0202  0.0149  0.0044   \n",
       "\n",
       "        589  target        date      time  \n",
       "0       NaN      -1  19/07/2008  11:55:00  \n",
       "1  208.2045      -1  19/07/2008  12:32:00  \n",
       "2   82.8602       1  19/07/2008  13:17:00  \n",
       "3   73.8432      -1  19/07/2008  14:43:00  \n",
       "4   73.8432      -1  19/07/2008  15:22:00  \n",
       "\n",
       "[5 rows x 593 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#join labels to secom dataset\n",
    "\n",
    "df = secom.join(labels)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning & Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our data set in one piece, we can now focus on the null values in the data set. Since we are looking to find the most predictive features, we will likely not find signal in columns with an excessive number of missing values. Let's drop columns that have a high percentage of the 1567 observations as null values, as we're highly unlikely to find any of these to be useful in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping columns with over 700 null values\n",
    "drop_na = list(df.loc[:,df.isna().sum() > 700].columns)\n",
    "df.drop(drop_na, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll try to separate categorical from numeric columns. One way to perform this step is to simply classify binary columns as categorical, as we know those are not likely to be numeric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 13, 42, 49, 52, 69, 97, 141, 149, 178, 179, 186, 189, 190, 191, 192, 193, 194, 226, 229, 230, 231, 232, 233, 234, 235, 236, 237, 240, 241, 242, 243, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 276, 284, 313, 314, 315, 322, 325, 326, 327, 328, 329, 330, 364, 369, 370, 371, 372, 373, 374, 375, 378, 379, 380, 381, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 414, 422, 449, 450, 451, 458, 461, 462, 463, 464, 465, 466, 481, 498, 501, 502, 503, 504, 505, 506, 507, 508, 509, 512, 513, 514, 515, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 'target']\n"
     ]
    }
   ],
   "source": [
    "#initial list\n",
    "#if unique values in columns are less than 3 append to new list\n",
    "\n",
    "indices = []\n",
    "\n",
    "for i in df.columns:\n",
    "    unique_vals = df[i].unique().shape[0]\n",
    "    if unique_vals < 3:\n",
    "        indices.append(i)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert indices type to category\n",
    "#classify columns as numeric and categorical\n",
    "\n",
    "df[indices] = df[indices].astype('category')\n",
    "cat_cols = df.select_dtypes(['category', 'object']).columns\n",
    "num_cols = df.select_dtypes(['integer', 'float']).columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now having classified binary columns as categorical, we can impute these columns using the mode of each feature. We'll use the mode to impute NAs in the categorical features and the median instead of mean for our numeric columns, as to not be skewed by outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in cat_cols:\n",
    "    df[column].fillna(df[column].mode()[0], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in num_cols:\n",
    "    df[column].fillna(df[column].median(), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before doing any feature selection, we should make sure to standardize the numeric features in our data. Measurements in our data are likely to have different units and scales, so to account for this, we must transform these features so they are on the same scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>583</th>\n",
       "      <th>584</th>\n",
       "      <th>585</th>\n",
       "      <th>586</th>\n",
       "      <th>587</th>\n",
       "      <th>588</th>\n",
       "      <th>589</th>\n",
       "      <th>target</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.224463</td>\n",
       "      <td>0.849523</td>\n",
       "      <td>-0.436430</td>\n",
       "      <td>0.035804</td>\n",
       "      <td>-0.050121</td>\n",
       "      <td>100.0</td>\n",
       "      <td>-0.564354</td>\n",
       "      <td>0.265894</td>\n",
       "      <td>0.509848</td>\n",
       "      <td>1.128455</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.204833</td>\n",
       "      <td>-0.093165</td>\n",
       "      <td>-0.197057</td>\n",
       "      <td>-0.077554</td>\n",
       "      <td>-0.190165</td>\n",
       "      <td>-0.238334</td>\n",
       "      <td>-0.295753</td>\n",
       "      <td>-1</td>\n",
       "      <td>19/07/2008</td>\n",
       "      <td>11:55:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.107287</td>\n",
       "      <td>-0.383106</td>\n",
       "      <td>1.016977</td>\n",
       "      <td>0.155282</td>\n",
       "      <td>-0.059585</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.197639</td>\n",
       "      <td>0.321868</td>\n",
       "      <td>0.457021</td>\n",
       "      <td>0.022620</td>\n",
       "      <td>...</td>\n",
       "      <td>0.406734</td>\n",
       "      <td>0.444748</td>\n",
       "      <td>0.385113</td>\n",
       "      <td>-0.960123</td>\n",
       "      <td>0.411970</td>\n",
       "      <td>0.250272</td>\n",
       "      <td>1.156846</td>\n",
       "      <td>-1</td>\n",
       "      <td>19/07/2008</td>\n",
       "      <td>12:32:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.114000</td>\n",
       "      <td>0.798901</td>\n",
       "      <td>-0.481447</td>\n",
       "      <td>0.688278</td>\n",
       "      <td>-0.047447</td>\n",
       "      <td>100.0</td>\n",
       "      <td>-0.906768</td>\n",
       "      <td>0.254699</td>\n",
       "      <td>-0.260885</td>\n",
       "      <td>0.327222</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022320</td>\n",
       "      <td>0.014418</td>\n",
       "      <td>0.029888</td>\n",
       "      <td>2.991195</td>\n",
       "      <td>3.627143</td>\n",
       "      <td>3.321511</td>\n",
       "      <td>-0.178955</td>\n",
       "      <td>1</td>\n",
       "      <td>19/07/2008</td>\n",
       "      <td>13:17:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.350156</td>\n",
       "      <td>-0.199072</td>\n",
       "      <td>-0.051705</td>\n",
       "      <td>-1.104376</td>\n",
       "      <td>-0.050831</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.502662</td>\n",
       "      <td>-0.013974</td>\n",
       "      <td>0.343240</td>\n",
       "      <td>-0.765369</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.292200</td>\n",
       "      <td>-0.362121</td>\n",
       "      <td>-0.283360</td>\n",
       "      <td>-0.101845</td>\n",
       "      <td>-0.178804</td>\n",
       "      <td>-0.308135</td>\n",
       "      <td>-0.275049</td>\n",
       "      <td>-1</td>\n",
       "      <td>19/07/2008</td>\n",
       "      <td>14:43:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 561 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4      5         6  \\\n",
       "0  0.224463  0.849523 -0.436430  0.035804 -0.050121  100.0 -0.564354   \n",
       "1  1.107287 -0.383106  1.016977  0.155282 -0.059585  100.0  0.197639   \n",
       "2 -1.114000  0.798901 -0.481447  0.688278 -0.047447  100.0 -0.906768   \n",
       "3 -0.350156 -0.199072 -0.051705 -1.104376 -0.050831  100.0  0.502662   \n",
       "\n",
       "          7         8         9  ...       583       584       585       586  \\\n",
       "0  0.265894  0.509848  1.128455  ... -0.204833 -0.093165 -0.197057 -0.077554   \n",
       "1  0.321868  0.457021  0.022620  ...  0.406734  0.444748  0.385113 -0.960123   \n",
       "2  0.254699 -0.260885  0.327222  ...  0.022320  0.014418  0.029888  2.991195   \n",
       "3 -0.013974  0.343240 -0.765369  ... -0.292200 -0.362121 -0.283360 -0.101845   \n",
       "\n",
       "        587       588       589  target        date      time  \n",
       "0 -0.190165 -0.238334 -0.295753      -1  19/07/2008  11:55:00  \n",
       "1  0.411970  0.250272  1.156846      -1  19/07/2008  12:32:00  \n",
       "2  3.627143  3.321511 -0.178955       1  19/07/2008  13:17:00  \n",
       "3 -0.178804 -0.308135 -0.275049      -1  19/07/2008  14:43:00  \n",
       "\n",
       "[4 rows x 561 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initialize standardscaler\n",
    "#fit numeric columns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "znormalizer = StandardScaler()\n",
    "znormalizer.fit(df[num_cols])\n",
    "df[num_cols] = pd.DataFrame(znormalizer.transform(df[num_cols]), columns = num_cols)\n",
    "df.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with Class Imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to check the distribution of our target variable before getting to feature selection and modeling. An imbalanced target can lead to biases in our predictions towards the majority class. \n",
    "\n",
    "In our case, we are dealing with defects in the manufacturing process. It's likely that the positive cases in such a problem are very low in proportion to the negative ones. Because we are more interested in the model identifying the minority class (when there is a defect in the manufacturing process), we need to pay special attention to the class imbalance problem in this data set. With a very small number of positive cases in the data set, the model we choose will likely find it challenging to predict the minority class in the future because of the lack of historical examples we are feeding it.\n",
    "\n",
    "Before we apply any methods to solve for class imbalance, let's visualize the target feature to get a better sense of the distribution of positive to negative cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(x = 'target', data = df);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is clearly a very imbalanced data set with less than 10% of the cases as positive. Before we take care of this, lets first remove more of the features that may not help us, starting with the date and time. These features are not likely to be more predictive than any of our sensor data.\n",
    "\n",
    "Next, we can remove zero variance features from the model. A higher variance provides the model with more information regarding the relationship between a feature and the target. Columns with a completely uniform set of values are not likely to have any importance to us and can safely be removed ahead of oversampling and feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop date and time\n",
    "\n",
    "del[df['date']]\n",
    "del[df['time']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('target', axis = 1)\n",
    "Y = df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import variance threshold package\n",
    "#default removes zero variance features\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "sel = VarianceThreshold()    \n",
    "X = sel.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>432</th>\n",
       "      <th>433</th>\n",
       "      <th>434</th>\n",
       "      <th>435</th>\n",
       "      <th>436</th>\n",
       "      <th>437</th>\n",
       "      <th>438</th>\n",
       "      <th>439</th>\n",
       "      <th>440</th>\n",
       "      <th>441</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.224463</td>\n",
       "      <td>0.849523</td>\n",
       "      <td>-0.43643</td>\n",
       "      <td>0.0358043</td>\n",
       "      <td>-0.0501211</td>\n",
       "      <td>-0.564354</td>\n",
       "      <td>0.265894</td>\n",
       "      <td>0.509848</td>\n",
       "      <td>1.12845</td>\n",
       "      <td>-0.381577</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.229797</td>\n",
       "      <td>-0.13552</td>\n",
       "      <td>0.118679</td>\n",
       "      <td>-0.204833</td>\n",
       "      <td>-0.093165</td>\n",
       "      <td>-0.197057</td>\n",
       "      <td>-0.0775541</td>\n",
       "      <td>-0.190165</td>\n",
       "      <td>-0.238334</td>\n",
       "      <td>-0.295753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.10729</td>\n",
       "      <td>-0.383106</td>\n",
       "      <td>1.01698</td>\n",
       "      <td>0.155282</td>\n",
       "      <td>-0.0595851</td>\n",
       "      <td>0.197639</td>\n",
       "      <td>0.321868</td>\n",
       "      <td>0.457021</td>\n",
       "      <td>0.0226205</td>\n",
       "      <td>-1.60828</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.263493</td>\n",
       "      <td>-0.460054</td>\n",
       "      <td>0.530183</td>\n",
       "      <td>0.406734</td>\n",
       "      <td>0.444748</td>\n",
       "      <td>0.385113</td>\n",
       "      <td>-0.960123</td>\n",
       "      <td>0.41197</td>\n",
       "      <td>0.250272</td>\n",
       "      <td>1.15685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.114</td>\n",
       "      <td>0.798901</td>\n",
       "      <td>-0.481447</td>\n",
       "      <td>0.688278</td>\n",
       "      <td>-0.0474466</td>\n",
       "      <td>-0.906768</td>\n",
       "      <td>0.254699</td>\n",
       "      <td>-0.260885</td>\n",
       "      <td>0.327222</td>\n",
       "      <td>0.124169</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.205046</td>\n",
       "      <td>-0.590505</td>\n",
       "      <td>-1.2628</td>\n",
       "      <td>0.0223203</td>\n",
       "      <td>0.0144176</td>\n",
       "      <td>0.0298878</td>\n",
       "      <td>2.99119</td>\n",
       "      <td>3.62714</td>\n",
       "      <td>3.32151</td>\n",
       "      <td>-0.178955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.350156</td>\n",
       "      <td>-0.199072</td>\n",
       "      <td>-0.051705</td>\n",
       "      <td>-1.10438</td>\n",
       "      <td>-0.0508307</td>\n",
       "      <td>0.502662</td>\n",
       "      <td>-0.0139737</td>\n",
       "      <td>0.34324</td>\n",
       "      <td>-0.765369</td>\n",
       "      <td>-0.370817</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.22495</td>\n",
       "      <td>-0.645708</td>\n",
       "      <td>-0.322218</td>\n",
       "      <td>-0.2922</td>\n",
       "      <td>-0.362121</td>\n",
       "      <td>-0.28336</td>\n",
       "      <td>-0.101845</td>\n",
       "      <td>-0.178804</td>\n",
       "      <td>-0.308135</td>\n",
       "      <td>-0.275049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.242296</td>\n",
       "      <td>0.0873275</td>\n",
       "      <td>1.11723</td>\n",
       "      <td>-0.156616</td>\n",
       "      <td>-0.0470329</td>\n",
       "      <td>-0.115954</td>\n",
       "      <td>0.187531</td>\n",
       "      <td>0.545066</td>\n",
       "      <td>-0.149545</td>\n",
       "      <td>-0.790478</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.230791</td>\n",
       "      <td>-0.454486</td>\n",
       "      <td>-5.90692</td>\n",
       "      <td>26.8672</td>\n",
       "      <td>27.0714</td>\n",
       "      <td>26.9133</td>\n",
       "      <td>-0.101845</td>\n",
       "      <td>-0.178804</td>\n",
       "      <td>-0.308135</td>\n",
       "      <td>-0.275049</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 442 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0          1         2          3          4         5          6    \\\n",
       "0  0.224463   0.849523  -0.43643  0.0358043 -0.0501211 -0.564354   0.265894   \n",
       "1   1.10729  -0.383106   1.01698   0.155282 -0.0595851  0.197639   0.321868   \n",
       "2    -1.114   0.798901 -0.481447   0.688278 -0.0474466 -0.906768   0.254699   \n",
       "3 -0.350156  -0.199072 -0.051705   -1.10438 -0.0508307  0.502662 -0.0139737   \n",
       "4  0.242296  0.0873275   1.11723  -0.156616 -0.0470329 -0.115954   0.187531   \n",
       "\n",
       "        7          8         9    ...       432       433       434  \\\n",
       "0  0.509848    1.12845 -0.381577  ... -0.229797  -0.13552  0.118679   \n",
       "1  0.457021  0.0226205  -1.60828  ... -0.263493 -0.460054  0.530183   \n",
       "2 -0.260885   0.327222  0.124169  ... -0.205046 -0.590505   -1.2628   \n",
       "3   0.34324  -0.765369 -0.370817  ...  -0.22495 -0.645708 -0.322218   \n",
       "4  0.545066  -0.149545 -0.790478  ... -0.230791 -0.454486  -5.90692   \n",
       "\n",
       "         435        436        437        438       439       440       441  \n",
       "0  -0.204833  -0.093165  -0.197057 -0.0775541 -0.190165 -0.238334 -0.295753  \n",
       "1   0.406734   0.444748   0.385113  -0.960123   0.41197  0.250272   1.15685  \n",
       "2  0.0223203  0.0144176  0.0298878    2.99119   3.62714   3.32151 -0.178955  \n",
       "3    -0.2922  -0.362121   -0.28336  -0.101845 -0.178804 -0.308135 -0.275049  \n",
       "4    26.8672    27.0714    26.9133  -0.101845 -0.178804 -0.308135 -0.275049  \n",
       "\n",
       "[5 rows x 442 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convert new array back into data frame\n",
    "\n",
    "X = pd.DataFrame(sel.fit_transform(X))\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, having removed over 100 unnecessary features, we can take care of the class imbalance in our data set. We can use SMOTE to over sample the minority class (in this case the defects). SMOTE will balance the number of observations in each class by manufacturing synthetic data using a kmeans clustering algorithm.\n",
    "\n",
    "SMOTE is a widely used approach to synthesizing new examples in a data set. It's useful to us in this case, because the algorithm can select a minority class at random, find its k nearest minority class neighbor, and then create a synthetic instance by choosing one of the k nearest neighbors at random to form a line segment in the feature space. After we apply SMOTE, we should see a balanced set of positive and negative cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "smote = SMOTE(random_state=0)\n",
    "X_smote, Y_smote = smote.fit_resample(X, Y)\n",
    "\n",
    "df = pd.concat([pd.DataFrame(X_smote), pd.DataFrame(Y_smote, columns = ['target'])], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(x = 'target', data = df);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like SMOTE did its job. We can now get into feature selection. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection\n",
    "\n",
    "We're going to start by doing a simple mutual information classification to give us a subset of important features to use in our predictive model. The reason for using mutual information is that it provides the important statistical dependencies that exist in our data set in the context of our target variable.\n",
    "\n",
    "The definition for mutual information is provided below:\n",
    "\n",
    "*A quantity called mutual information measures the amount of information one can obtain from one random variable given another*\n",
    "\n",
    "Running a mutual information classification is an efficient way of selecting a set of features that can explain the target variable from a larger data set. After running this, we can use the subset of features chosen by the mutual information classification and run stepwise feature selection to iteratively select a smaller set of predictive features for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prep for mutual info classification and feature selection\n",
    "X = df\n",
    "y = df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(442, 0.6933180915103432),\n",
       " (294, 0.46870695537474116),\n",
       " (436, 0.46299664863045376),\n",
       " (192, 0.4596942970684428),\n",
       " (148, 0.45752637073357527),\n",
       " (107, 0.4535821123996915),\n",
       " (110, 0.4534982012291622),\n",
       " (94, 0.4531605699911585),\n",
       " (128, 0.44667230744107167),\n",
       " (139, 0.4463279714785202),\n",
       " (229, 0.44407258071961975),\n",
       " (291, 0.44364849369650927),\n",
       " (295, 0.4417123176595654),\n",
       " (201, 0.4416436873067744),\n",
       " (293, 0.4398782170889084),\n",
       " (284, 0.4358190367813959),\n",
       " (149, 0.434040649315383),\n",
       " (117, 0.432839840205546),\n",
       " (292, 0.4314171330003582),\n",
       " (200, 0.4310438512718602),\n",
       " (302, 0.42973079543930415),\n",
       " (83, 0.42905154060885264),\n",
       " (109, 0.4281289103245358),\n",
       " (224, 0.427867308722913),\n",
       " (115, 0.42410377591798376),\n",
       " (102, 0.4230359923723772),\n",
       " (286, 0.4220921021508375),\n",
       " (440, 0.4219290756162788),\n",
       " (6, 0.4200021719235896),\n",
       " (290, 0.4194535887965116),\n",
       " (220, 0.41848667341226675),\n",
       " (95, 0.416638255878798),\n",
       " (96, 0.41147109214001154),\n",
       " (121, 0.41016067578472293),\n",
       " (222, 0.4098428034807773),\n",
       " (111, 0.40590108092789756),\n",
       " (90, 0.40316649859530784),\n",
       " (84, 0.4005196761260823),\n",
       " (50, 0.391967518768662),\n",
       " (27, 0.39176834778583225),\n",
       " (93, 0.3890454488900841),\n",
       " (296, 0.3849185700064226),\n",
       " (435, 0.384076083213289),\n",
       " (434, 0.3821366818905043),\n",
       " (301, 0.38070425647139783),\n",
       " (205, 0.3769404586186085),\n",
       " (76, 0.37604059519585586),\n",
       " (91, 0.37294592529558446),\n",
       " (49, 0.37244669162046895),\n",
       " (194, 0.370200528103269),\n",
       " (223, 0.36624266073743295),\n",
       " (146, 0.36607474188768463),\n",
       " (51, 0.36485533933658343),\n",
       " (150, 0.360948235607111),\n",
       " (38, 0.36028621123384763),\n",
       " (31, 0.3602711897789552),\n",
       " (85, 0.35994107069179826),\n",
       " (48, 0.35865251655161057),\n",
       " (52, 0.3583304864475796),\n",
       " (116, 0.35760940307784206),\n",
       " (246, 0.35732777147847017),\n",
       " (89, 0.35644616850567656),\n",
       " (248, 0.3518368023437195),\n",
       " (34, 0.3515568051463376),\n",
       " (137, 0.3513879246717486),\n",
       " (32, 0.3508416637940708),\n",
       " (112, 0.34815467338308626),\n",
       " (247, 0.3449678522363553),\n",
       " (251, 0.3429879333050978),\n",
       " (37, 0.3414251680516711),\n",
       " (108, 0.3408818374061171),\n",
       " (341, 0.3407798665066193),\n",
       " (212, 0.3403192159112569),\n",
       " (33, 0.34008610970947006),\n",
       " (36, 0.33999239721797925),\n",
       " (249, 0.3392978282981003),\n",
       " (35, 0.33914145061228584),\n",
       " (407, 0.33797970019266366),\n",
       " (119, 0.33778478530420797),\n",
       " (120, 0.33391633826247413),\n",
       " (306, 0.33381367041705956),\n",
       " (154, 0.3331088970105056),\n",
       " (285, 0.3324537019363001),\n",
       " (345, 0.3318044146334178),\n",
       " (344, 0.33159402058398757),\n",
       " (158, 0.33135166579974373),\n",
       " (153, 0.3311543042134564),\n",
       " (113, 0.32997273108208414),\n",
       " (168, 0.3297994712573573),\n",
       " (304, 0.32928871368991963),\n",
       " (156, 0.32859046217453347),\n",
       " (145, 0.3266858252695144),\n",
       " (30, 0.32651988817020494),\n",
       " (342, 0.32531784838241706),\n",
       " (397, 0.3244971325734962),\n",
       " (325, 0.3242400719874947),\n",
       " (199, 0.32374723700892694),\n",
       " (250, 0.32182957217131514),\n",
       " (78, 0.32085546722078306),\n",
       " (338, 0.3203934860151241)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#run classification\n",
    "#convert output to list and show 100 highest mutual information features\n",
    "\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "import heapq\n",
    "\n",
    "mi_score = mutual_info_classif(np.array(X), y)\n",
    "\n",
    "mi = mi_score.tolist()\n",
    "num = 100\n",
    "\n",
    "heapq.nlargest(num, enumerate(mi), key = lambda mi: mi[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have the 100 features with the highest mutual information. We can use these features to subset our original data set before using a step wise feature selection method called recursive feature elimination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use feature names to subset original dataframe\n",
    "\n",
    "m = heapq.nlargest(num, enumerate(mi), key = lambda mi: mi[1])\n",
    "X_col = [i[0] for i in m]\n",
    "X_col = pd.DataFrame(np.array(X_col, ndmin = 2))\n",
    "X_col = X_col.rename(columns = X_col.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>294</th>\n",
       "      <th>436</th>\n",
       "      <th>192</th>\n",
       "      <th>148</th>\n",
       "      <th>107</th>\n",
       "      <th>110</th>\n",
       "      <th>94</th>\n",
       "      <th>128</th>\n",
       "      <th>139</th>\n",
       "      <th>229</th>\n",
       "      <th>...</th>\n",
       "      <th>156</th>\n",
       "      <th>145</th>\n",
       "      <th>30</th>\n",
       "      <th>342</th>\n",
       "      <th>397</th>\n",
       "      <th>325</th>\n",
       "      <th>199</th>\n",
       "      <th>250</th>\n",
       "      <th>78</th>\n",
       "      <th>338</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.12549</td>\n",
       "      <td>-0.093165</td>\n",
       "      <td>-1.37371</td>\n",
       "      <td>-0.704959</td>\n",
       "      <td>0.842009</td>\n",
       "      <td>1.18979</td>\n",
       "      <td>0.37018</td>\n",
       "      <td>-0.60092</td>\n",
       "      <td>-0.0858026</td>\n",
       "      <td>0.537776</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.0240888</td>\n",
       "      <td>-0.411725</td>\n",
       "      <td>-0.958019</td>\n",
       "      <td>-0.01624</td>\n",
       "      <td>0.656685</td>\n",
       "      <td>-0.0882377</td>\n",
       "      <td>3.42421</td>\n",
       "      <td>1.58996</td>\n",
       "      <td>-1.04489</td>\n",
       "      <td>0.365113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.12238</td>\n",
       "      <td>0.444748</td>\n",
       "      <td>1.06619</td>\n",
       "      <td>-0.526181</td>\n",
       "      <td>0.842009</td>\n",
       "      <td>0.750364</td>\n",
       "      <td>0.252518</td>\n",
       "      <td>0.572904</td>\n",
       "      <td>-0.139442</td>\n",
       "      <td>0.333164</td>\n",
       "      <td>...</td>\n",
       "      <td>1.28347</td>\n",
       "      <td>-0.231211</td>\n",
       "      <td>-0.213375</td>\n",
       "      <td>1.32406</td>\n",
       "      <td>-0.0108205</td>\n",
       "      <td>-0.132235</td>\n",
       "      <td>0.536435</td>\n",
       "      <td>-0.408111</td>\n",
       "      <td>0.543432</td>\n",
       "      <td>2.35919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.935059</td>\n",
       "      <td>0.0144176</td>\n",
       "      <td>-1.12972</td>\n",
       "      <td>-0.61557</td>\n",
       "      <td>1.04342</td>\n",
       "      <td>1.36556</td>\n",
       "      <td>-1.86539</td>\n",
       "      <td>-0.2879</td>\n",
       "      <td>-0.0321631</td>\n",
       "      <td>0.674184</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.0844808</td>\n",
       "      <td>0.384177</td>\n",
       "      <td>-0.2866</td>\n",
       "      <td>-0.120376</td>\n",
       "      <td>-0.10604</td>\n",
       "      <td>-0.0436085</td>\n",
       "      <td>-0.0783813</td>\n",
       "      <td>0.700382</td>\n",
       "      <td>0.43443</td>\n",
       "      <td>2.58027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.49703</td>\n",
       "      <td>-0.362121</td>\n",
       "      <td>-0.580741</td>\n",
       "      <td>-0.973127</td>\n",
       "      <td>-2.48124</td>\n",
       "      <td>-1.79832</td>\n",
       "      <td>0.37018</td>\n",
       "      <td>-0.91394</td>\n",
       "      <td>-0.139442</td>\n",
       "      <td>0.333164</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.819016</td>\n",
       "      <td>-0.592239</td>\n",
       "      <td>-0.210758</td>\n",
       "      <td>-0.836574</td>\n",
       "      <td>-0.635529</td>\n",
       "      <td>-0.132603</td>\n",
       "      <td>-0.274005</td>\n",
       "      <td>-0.00154785</td>\n",
       "      <td>0.621291</td>\n",
       "      <td>1.6476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.68435</td>\n",
       "      <td>27.0714</td>\n",
       "      <td>-0.153759</td>\n",
       "      <td>0.993435</td>\n",
       "      <td>-0.467148</td>\n",
       "      <td>-0.74369</td>\n",
       "      <td>0.252518</td>\n",
       "      <td>-0.0531354</td>\n",
       "      <td>-0.0679228</td>\n",
       "      <td>0.0262457</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.208074</td>\n",
       "      <td>0.0559701</td>\n",
       "      <td>0.488609</td>\n",
       "      <td>-0.237263</td>\n",
       "      <td>-0.0451341</td>\n",
       "      <td>-0.0719771</td>\n",
       "      <td>0.871789</td>\n",
       "      <td>-1.36023</td>\n",
       "      <td>0.60572</td>\n",
       "      <td>-0.732689</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 99 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        294        436       192       148       107       110       94   \\\n",
       "0  -1.12549  -0.093165  -1.37371 -0.704959  0.842009   1.18979   0.37018   \n",
       "1   1.12238   0.444748   1.06619 -0.526181  0.842009  0.750364  0.252518   \n",
       "2  0.935059  0.0144176  -1.12972  -0.61557   1.04342   1.36556  -1.86539   \n",
       "3   1.49703  -0.362121 -0.580741 -0.973127  -2.48124  -1.79832   0.37018   \n",
       "4   1.68435    27.0714 -0.153759  0.993435 -0.467148  -0.74369  0.252518   \n",
       "\n",
       "         128        139        229  ...        156        145       30   \\\n",
       "0   -0.60092 -0.0858026   0.537776  ... -0.0240888  -0.411725 -0.958019   \n",
       "1   0.572904  -0.139442   0.333164  ...    1.28347  -0.231211 -0.213375   \n",
       "2    -0.2879 -0.0321631   0.674184  ... -0.0844808   0.384177   -0.2866   \n",
       "3   -0.91394  -0.139442   0.333164  ...  -0.819016  -0.592239 -0.210758   \n",
       "4 -0.0531354 -0.0679228  0.0262457  ...  -0.208074  0.0559701  0.488609   \n",
       "\n",
       "        342        397        325        199         250       78        338  \n",
       "0  -0.01624   0.656685 -0.0882377    3.42421     1.58996  -1.04489  0.365113  \n",
       "1   1.32406 -0.0108205  -0.132235   0.536435   -0.408111  0.543432   2.35919  \n",
       "2 -0.120376   -0.10604 -0.0436085 -0.0783813    0.700382   0.43443   2.58027  \n",
       "3 -0.836574  -0.635529  -0.132603  -0.274005 -0.00154785  0.621291    1.6476  \n",
       "4 -0.237263 -0.0451341 -0.0719771   0.871789    -1.36023   0.60572 -0.732689  \n",
       "\n",
       "[5 rows x 99 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#drop target from X\n",
    "#subset data frame to selected features by mutual info classification\n",
    "\n",
    "X_col = X_col.astype('object').columns.drop([442])\n",
    "X = X[X_col]\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to use recursive feature selection with logistic regression to choose the best features from our subset dataframe, optimizing for accuracy. We will choose 30 of the 100 features that are the most predictive of our target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kumar\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:70: FutureWarning: Pass n_features_to_select=30 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "C:\\Users\\kumar\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\kumar\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\kumar\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\kumar\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\kumar\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\kumar\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\kumar\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\kumar\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\kumar\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\kumar\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\kumar\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\kumar\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\kumar\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\kumar\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\kumar\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\kumar\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\kumar\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\kumar\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\kumar\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kumar\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\kumar\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\kumar\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\kumar\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\kumar\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\kumar\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\kumar\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\kumar\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\kumar\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\kumar\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\kumar\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\kumar\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\kumar\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\kumar\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\kumar\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\kumar\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\kumar\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kumar\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\kumar\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\kumar\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\kumar\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\kumar\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\kumar\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\kumar\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\kumar\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\kumar\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\kumar\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\kumar\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\kumar\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\kumar\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\kumar\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\kumar\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\kumar\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kumar\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\kumar\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\kumar\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\kumar\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\kumar\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\kumar\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\kumar\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\kumar\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\kumar\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\kumar\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\kumar\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\kumar\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\kumar\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\kumar\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\kumar\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\kumar\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\kumar\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "# select 30 independent features, removing 1 feature at a time\n",
    "# import logistic regression\n",
    "# Recursive Feature Elimination\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import RFE \n",
    "\n",
    "estimator = LogisticRegression()\n",
    "\n",
    "back_selector = RFE(estimator, 30, step = 1)\n",
    "back_selector = back_selector.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now output the results and see which features RFE chose for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4     True\n",
       "7     True\n",
       "8     True\n",
       "9     True\n",
       "10    True\n",
       "11    True\n",
       "12    True\n",
       "17    True\n",
       "20    True\n",
       "23    True\n",
       "25    True\n",
       "28    True\n",
       "30    True\n",
       "31    True\n",
       "33    True\n",
       "36    True\n",
       "48    True\n",
       "50    True\n",
       "51    True\n",
       "63    True\n",
       "71    True\n",
       "72    True\n",
       "74    True\n",
       "84    True\n",
       "90    True\n",
       "91    True\n",
       "93    True\n",
       "94    True\n",
       "95    True\n",
       "96    True\n",
       "Name: 0, dtype: bool"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#show features selected by RFE\n",
    "\n",
    "back_features = pd.DataFrame(back_selector.support_)\n",
    "\n",
    "sel_back = back_features[0].loc[back_features[0] == True]\n",
    "sel_back"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Takeaways\n",
    "\n",
    "As we are dealing with the complex manufacturing process of diapers and unlabeled sensor data, interpretability is certainly a challenge. With almost 600 independent features, it was important to quickly understand what was *not* going to be helpful in predicting whether there would be defects in the manufacturing process.\n",
    "\n",
    "In this analysis, we were able to identify over 100 features with uniform values that could safely be removed from the data set. These features were not going to help us explain the target and served as a good first step in bringing down the dimensionality of the data set.\n",
    "\n",
    "Also, while imputing missing values into the data set, we were able to see that nearly 40 of these features had over 700 (out of the 1567) observations missing. Imputing a uniform median or mode into these features would not help us predict the target. Therefore, these features could also be removed safely.\n",
    "\n",
    "After scaling the data, we then looked at the distribution of the target variable. This is a business scenario where class imbalance is highly probable. Defects in the manufacturing process of diapers are likely to be rare and this is also a class that the business is more interested in understanding and predicting. We found that less than 10% of the total cases were positive and therefore, had to use SMOTE to oversample the minority class, leaving us with a more balanced target variable ready for us to do feature selection.\n",
    "\n",
    "In the feature selection process, we found mutual information classification to be a simple method to bring down the feature set to a lower dimensionality, while leaving us with set of features that are likely to be predictive. We could then use the subset of these features in our stepwise feature selection. We set the number of features the model should select at 30 and iteratively went through each independent feature, leaving us with the most important set that we can now use for further analysis and prediction. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
